{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam\n",
      "0     Subject: naturally irresistible your corporate...     1\n",
      "1     Subject: the stock trading gunslinger  fanny i...     1\n",
      "2     Subject: unbelievable new homes made easy  im ...     1\n",
      "3     Subject: 4 color printing special  request add...     1\n",
      "4     Subject: do not have money , get software cds ...     1\n",
      "...                                                 ...   ...\n",
      "5723  Subject: re : research and development charges...     0\n",
      "5724  Subject: re : receipts from visit  jim ,  than...     0\n",
      "5725  Subject: re : enron case study update  wow ! a...     0\n",
      "5726  Subject: re : interest  david ,  please , call...     0\n",
      "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0\n",
      "\n",
      "[5728 rows x 2 columns]\n",
      "0       Subject: naturally irresistible your corporate...\n",
      "1       Subject: the stock trading gunslinger  fanny i...\n",
      "2       Subject: unbelievable new homes made easy  im ...\n",
      "3       Subject: 4 color printing special  request add...\n",
      "4       Subject: do not have money , get software cds ...\n",
      "                              ...                        \n",
      "5723    Subject: re : research and development charges...\n",
      "5724    Subject: re : receipts from visit  jim ,  than...\n",
      "5725    Subject: re : enron case study update  wow ! a...\n",
      "5726    Subject: re : interest  david ,  please , call...\n",
      "5727    Subject: news : aurora 5 . 2 update  aurora ve...\n",
      "Name: text, Length: 5728, dtype: object\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "5723    0\n",
      "5724    0\n",
      "5725    0\n",
      "5726    0\n",
      "5727    0\n",
      "Name: spam, Length: 5728, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## ouvrir le csv avec pandas\n",
    "import pandas as pd\n",
    "data_spam =pd.read_csv(\"spamham.csv\")\n",
    "print(data_spam)\n",
    "print(data_spam[\"text\"])\n",
    "print(data_spam[\"spam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Récupérer les instances (X) et les classes (y) et vectoriser\n",
    "y = data_spam[\"spam\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "V = CountVectorizer(ngram_range = (1,2) )\n",
    "X = V.fit_transform(data_spam[\"text\"])\n",
    "\n",
    "## séparer train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bons résultats 1685\n",
      "Erreurs: 34\n"
     ]
    }
   ],
   "source": [
    "#classifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=0)\n",
    "ppn.fit(X_train, y_train)\n",
    "y_pred = ppn.predict(X_test)\n",
    "\n",
    "# On fait la somme de tous les cas où la valeur dans y_test est bien trouvée dans y_pred\n",
    "print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "print('Erreurs: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Réf: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "## à explorer: \n",
    "##   ngram_range = (min, max), par défaut (1,1)\n",
    "##   stop_words\n",
    "##   lowercase\n",
    "##   max_features\n",
    "#--> 1-grams\n",
    "#--> 2-grams\n",
    "# ce sont des approches sac de mots (BOW: Bag of Words)\n",
    "# on ajoutera à notre séance suivante des caractéristiques plus riches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avec plusieurs classifieurs\n",
    "##Comme les choses se répêtent, on va factoriser tout ça :\n",
    "## On fait tous les imports, on groupe les classifieurs, et on fait une boucle :\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "liste_classifieurs= [\n",
    "    [\"Perceptron\", Perceptron(eta0=0.1, random_state=0)],\n",
    "    [\"Logistic Regression\", LogisticRegression()],\n",
    "    [\"Decision Tree\", DecisionTreeClassifier()],\n",
    "    [\"SVM\", SVC(gamma=\"scale\")],\n",
    "    [\"linear_svc\", SVC(kernel='linear')],#Plus puissant que le SVC de base mais aussi plus long\n",
    "#    [\"KNN\", KNeighborsClassifier()]\n",
    "]\n",
    "## Pour éviter les warnings (merci julien):\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expériences stockées : 15\n"
     ]
    }
   ],
   "source": [
    "## chargement des expériences déjà faites\n",
    "import json\n",
    "import os\n",
    "chemin_expes = \"dic_expes_spams.json\"\n",
    "if os.path.exists(chemin_expes):\n",
    "    f = open(chemin_expes)\n",
    "    dic_expes = json.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    dic_expes = {}\n",
    "print(\"Expériences stockées : %s\"%len(dic_expes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Ngram_range : (1, 1)\n",
      "  Déjà vu\n",
      "   ['Perceptron', 1, 1, False, False] 0.954043048283886\n",
      "  Déjà vu\n",
      "   ['Logistic Regression', 1, 1, False, False] 0.9837114601512508\n",
      "  Déjà vu\n",
      "   ['Decision Tree', 1, 1, False, False] 0.9883653286794648\n",
      "  SVM classifier : 0.9540\n",
      "  linear_svc classifier : 0.9872\n",
      "  rbf_svc classifier : 0.9540\n"
     ]
    }
   ],
   "source": [
    "en_minuscules,enlever_stopwords  = False, False\n",
    "\n",
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 2):\n",
    "    V = CountVectorizer(ngram_range = (min_N, max_N))\n",
    "    X = V.fit_transform(data_spam[\"text\"])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print( \"-\" * 15)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "        expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules])\n",
    "        if expe in dic_expes:\n",
    "            print(\"  Déjà vu\")\n",
    "            print(\"  \",expe, score)\n",
    "            score = dic_expes[expe]\n",
    "        else:\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = score\n",
    "            print('  %s classifier : %.4f'%(nom, score))\n",
    "            pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BILAN: les 1 grammes ça a l'air d'être le mieux hors autres pré-traitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords False, Minuscules : False\n",
      "  Déjà vu : ['Perceptron', 1, 1, False, False, 100] 0.941826643397324\n",
      "  Déjà vu : ['Logistic Regression', 1, 1, False, False, 100] 0.9435718440954043\n",
      "  Déjà vu : ['Decision Tree', 1, 1, False, False, 100] 0.9488074461896452\n",
      "Stopwords False, Minuscules : True\n",
      "  Déjà vu : ['Perceptron', 1, 1, False, True, 100] 0.9360093077370564\n",
      "  Déjà vu : ['Logistic Regression', 1, 1, False, True, 100] 0.8900523560209425\n",
      "  Déjà vu : ['Decision Tree', 1, 1, False, True, 100] 0.9511343804537522\n",
      "Stopwords True, Minuscules : False\n",
      "  Déjà vu : ['Perceptron', 1, 1, True, False, 100] 0.9412449098312973\n",
      "  Déjà vu : ['Logistic Regression', 1, 1, True, False, 100] 0.9488074461896452\n",
      "  Déjà vu : ['Decision Tree', 1, 1, True, False, 100] 0.9505526468877254\n",
      "Stopwords True, Minuscules : True\n",
      "  Déjà vu : ['Perceptron', 1, 1, True, True, 100] 0.9400814426992438\n",
      "  Déjà vu : ['Logistic Regression', 1, 1, True, True, 100] 0.9383362420011635\n",
      "  Déjà vu : ['Decision Tree', 1, 1, True, True, 100] 0.9511343804537522\n",
      "--------------------\n",
      "Expériences stockées dans dic_expes_spams.json\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "min_N, max_N = 1, 1\n",
    "\n",
    "for enlever_stopwords in [False, True]:\n",
    "  liste_stopwords = None\n",
    "  if enlever_stopwords==True:\n",
    "    liste_stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "    \n",
    "  for en_minuscules in [False, True]:\n",
    "    print(f\"Stopwords {enlever_stopwords}, Minuscules : {en_minuscules}\")\n",
    "    for max_F in [100]:\n",
    "        V = CountVectorizer(lowercase = en_minuscules, stop_words =  liste_stopwords, max_features = max_F )\n",
    "        X = V.fit_transform(data_spam[\"text\"])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "        for nom, algo in liste_classifieurs:\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            expe = str([nom, min_N, max_N, enlever_stopwords, en_minuscules, max_F])\n",
    "            if expe in dic_expes:\n",
    "                print(\"  Déjà vu :\",  expe, score)\n",
    "                score = dic_expes[expe]\n",
    "            else:\n",
    "                print(\"  Nouvelle expérience\")\n",
    "                print('  %s classifier : %.4f'%(nom, score))\n",
    "                score = clf.score(X_test, y_test)\n",
    "                dic_expes[expe] = score\n",
    "\n",
    "w = open(chemin_expes, \"w\")\n",
    "w.write(json.dumps(dic_expes))\n",
    "w.close()\n",
    "print(\"-\"*20)\n",
    "print(f\"Expériences stockées dans {chemin_expes}\")\n",
    "print(\"-\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8900523560209425, ('Perceptron', 1, 1, False, True, 100)]\n",
      "[0.924956369982548, ('Decision Tree', 2, 2, False, False)]\n",
      "[0.93717277486911, ('Decision Tree', 1, 1, False, True, 1000)]\n",
      "[0.9377545084351367, ('Decision Tree', 1, 1, False, False, 100)]\n",
      "[0.9383362420011635, ('Perceptron', 1, 1, True, True, 100)]\n",
      "[0.9400814426992438, ('Decision Tree', 1, 1, False, False)]\n",
      "[0.9412449098312973, ('Decision Tree', 1, 1, True, False, 100)]\n",
      "[0.9412449098312973, ('Decision Tree', 1, 1, True, True, 100)]\n",
      "[0.941826643397324, ('Decision Tree', 1, 1, False, True, 100)]\n",
      "[0.9435718440954043, ('Perceptron', 1, 1, False, False, 100)]\n",
      "[0.9464805119255381, ('Decision Tree', 1, 1, False, False, 1000)]\n",
      "[0.9482257126236184, ('Decision Tree', 1, 1, False, True)]\n",
      "[0.9488074461896452, ('Decision Tree', 1, 1, False, True, 10000)]\n",
      "[0.9488074461896452, ('Decision Tree', 1, 2, False, False)]\n",
      "[0.9488074461896452, ('Logistic Regression', 1, 1, False, False, 100)]\n",
      "[0.9488074461896452, ('Perceptron', 1, 1, True, False, 100)]\n",
      "[0.9505526468877254, ('Decision Tree', 1, 1, True, False)]\n",
      "[0.9505526468877254, ('Logistic Regression', 1, 1, True, False, 100)]\n",
      "[0.9511343804537522, ('Decision Tree', 1, 1, True, True)]\n",
      "[0.9511343804537522, ('Logistic Regression', 1, 1, False, True, 100)]\n",
      "[0.9511343804537522, ('Logistic Regression', 1, 1, True, True, 100)]\n",
      "[0.951716114019779, ('Decision Tree', 1, 1, False, False, 10000)]\n",
      "[0.954043048283886, ('Decision Tree', 1, 1, True, False, 1000)]\n",
      "[0.954043048283886, ('Decision Tree', 1, 1, True, False, 10000)]\n",
      "[0.954043048283886, ('Decision Tree', 1, 1, True, True, 10000)]\n",
      "[0.9616055846422339, ('Decision Tree', 1, 1, True, True, 1000)]\n",
      "[0.9697498545666084, ('Logistic Regression', 2, 2, False, False)]\n",
      "[0.970913321698662, ('Perceptron', 2, 2, False, False)]\n",
      "[0.9738219895287958, ('Perceptron', 1, 1, True, False, 1000)]\n",
      "[0.9755671902268761, ('Perceptron', 1, 1, True, True, 1000)]\n",
      "[0.9773123909249564, ('Perceptron', 1, 1, False, False, 1000)]\n",
      "[0.9802210587550901, ('Perceptron', 1, 2, False, False)]\n",
      "[0.9813845258871436, ('Perceptron', 1, 1, True, False, 10000)]\n",
      "[0.9819662594531704, ('Perceptron', 1, 1, False, False, 10000)]\n",
      "[0.9819662594531704, ('Perceptron', 1, 1, False, True, 10000)]\n",
      "[0.983129726585224, ('Logistic Regression', 1, 1, True, False, 1000)]\n",
      "[0.983129726585224, ('Logistic Regression', 1, 1, True, True, 1000)]\n",
      "[0.983129726585224, ('Perceptron', 1, 1, False, True, 1000)]\n",
      "[0.983129726585224, ('Perceptron', 1, 1, True, False)]\n",
      "[0.9837114601512508, ('Perceptron', 1, 1, False, False)]\n",
      "[0.9837114601512508, ('Perceptron', 1, 1, False, True)]\n",
      "[0.9842931937172775, ('Perceptron', 1, 1, True, True)]\n",
      "[0.9866201279813845, ('Logistic Regression', 1, 1, False, False, 1000)]\n",
      "[0.9866201279813845, ('Logistic Regression', 1, 1, False, True, 1000)]\n",
      "[0.9872018615474113, ('Logistic Regression', 1, 2, False, False)]\n",
      "[0.9872018615474113, ('Perceptron', 1, 1, True, True, 10000)]\n",
      "[0.987783595113438, ('Logistic Regression', 1, 1, True, False)]\n",
      "[0.987783595113438, ('Logistic Regression', 1, 1, True, True)]\n",
      "[0.9883653286794648, ('Logistic Regression', 1, 1, False, False)]\n",
      "[0.9883653286794648, ('Logistic Regression', 1, 1, False, False, 10000)]\n",
      "[0.9883653286794648, ('Logistic Regression', 1, 1, False, True)]\n",
      "[0.9883653286794648, ('Logistic Regression', 1, 1, False, True, 10000)]\n",
      "[0.9901105293775451, ('Logistic Regression', 1, 1, True, False, 10000)]\n",
      "[0.9901105293775451, ('Logistic Regression', 1, 1, True, True, 10000)]\n"
     ]
    }
   ],
   "source": [
    "liste_resultats =[[score, nom_expe] for nom_expe, score in dic_expes.items()]\n",
    "for res in sorted(liste_resultats):\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram_range : (4, 4)\n",
      "Logistic Regression classifier : 0.9889\n",
      "Ngram_range : (4, 5)\n",
      "Logistic Regression classifier : 0.9889\n",
      "Ngram_range : (4, 6)\n",
      "Logistic Regression classifier : 0.9895\n"
     ]
    }
   ],
   "source": [
    "liste_classifieurs= [\n",
    "    [\"Logistic Regression\", LogisticRegression()]]\n",
    "for min_N in range(4, 5):\n",
    "  for max_N in range(min_N, 7):\n",
    "    #analyzer: words, char, char_wb\n",
    "    V = CountVectorizer(ngram_range = (min_N, max_N), analyzer = \"char\")\n",
    "    X = V.fit_transform(data_spam[\"text\"])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for nom, algo in liste_classifieurs:\n",
    "            clf = algo.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            dic_expes[expe] = score\n",
    "            print('%s classifier : %.4f'%(nom, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
